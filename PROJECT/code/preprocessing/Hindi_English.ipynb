{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z67c7FeL9m3M",
        "outputId": "4e62d9e9-68e5-4548-ee85-e767b46138fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1m1sQHXBgBR9",
        "outputId": "252ef924-2b9f-4aa3-f0be-e211eceb901a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting googletrans==4.0.0-rc1\n",
            "  Downloading googletrans-4.0.0rc1.tar.gz (20 kB)\n",
            "Collecting httpx==0.13.3\n",
            "  Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 2.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet==3.* in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (3.0.4)\n",
            "Collecting rfc3986<2,>=1.3\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Collecting httpcore==0.9.*\n",
            "  Downloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n",
            "\u001b[K     |████████████████████████████████| 42 kB 1.2 MB/s \n",
            "\u001b[?25hCollecting hstspreload\n",
            "  Downloading hstspreload-2021.11.1-py3-none-any.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 18.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2021.10.8)\n",
            "Collecting sniffio\n",
            "  Downloading sniffio-1.2.0-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: idna==2.* in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2.10)\n",
            "Collecting h2==3.*\n",
            "  Downloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
            "\u001b[K     |████████████████████████████████| 65 kB 3.3 MB/s \n",
            "\u001b[?25hCollecting h11<0.10,>=0.8\n",
            "  Downloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
            "\u001b[K     |████████████████████████████████| 53 kB 1.9 MB/s \n",
            "\u001b[?25hCollecting hpack<4,>=3.0\n",
            "  Downloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
            "Collecting hyperframe<6,>=5.2.0\n",
            "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
            "Building wheels for collected packages: googletrans\n",
            "  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletrans: filename=googletrans-4.0.0rc1-py3-none-any.whl size=17415 sha256=5fa778ebe58f100a6f449f85c5627bed4a300c9deb361b231a0f9a415581ae4f\n",
            "  Stored in directory: /root/.cache/pip/wheels/43/34/00/4fe71786ea6d12314b29037620c36d857e5d104ac2748bf82a\n",
            "Successfully built googletrans\n",
            "Installing collected packages: hyperframe, hpack, sniffio, h2, h11, rfc3986, httpcore, hstspreload, httpx, googletrans\n",
            "Successfully installed googletrans-4.0.0rc1 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2021.11.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 rfc3986-1.5.0 sniffio-1.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install googletrans==4.0.0-rc1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OBk_g8lPbEu1",
        "outputId": "1faa03ec-2bfe-49bc-86bc-e30606418848"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pygoogletranslation\n",
            "  Downloading pygoogletranslation-2.0.6-py3-none-any.whl (15 kB)\n",
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.3.2-py3-none-any.whl (235 kB)\n",
            "\u001b[K     |████████████████████████████████| 235 kB 10.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from pygoogletranslation) (3.2.5)\n",
            "Collecting docx2txt\n",
            "  Downloading docx2txt-0.8.tar.gz (2.8 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pygoogletranslation) (2.23.0)\n",
            "Collecting PyPDF2\n",
            "  Downloading PyPDF2-1.26.0.tar.gz (77 kB)\n",
            "\u001b[K     |████████████████████████████████| 77 kB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->pygoogletranslation) (1.15.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->pygoogletranslation) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pygoogletranslation) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pygoogletranslation) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pygoogletranslation) (2021.10.8)\n",
            "Building wheels for collected packages: docx2txt, PyPDF2\n",
            "  Building wheel for docx2txt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docx2txt: filename=docx2txt-0.8-py3-none-any.whl size=3980 sha256=16cde6ed860454a28fc5d46bc2a7171c5c29cdb1a661bdc82d2414b0b4a592e0\n",
            "  Stored in directory: /root/.cache/pip/wheels/b7/20/b2/473e3aea9a0c0d3e7b2f7bd81d06d0794fec12752733d1f3a8\n",
            "  Building wheel for PyPDF2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PyPDF2: filename=PyPDF2-1.26.0-py3-none-any.whl size=61101 sha256=dc1258914b4fb13e16df79873ce01c6cdc82ae1c3befe6d709b3c5a161ab9662\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1a/24/648467ade3a77ed20f35cfd2badd32134e96dd25ca811e64b3\n",
            "Successfully built docx2txt PyPDF2\n",
            "Installing collected packages: unidecode, PyPDF2, docx2txt, pygoogletranslation\n",
            "Successfully installed PyPDF2-1.26.0 docx2txt-0.8 pygoogletranslation-2.0.6 unidecode-1.3.2\n"
          ]
        }
      ],
      "source": [
        "pip install pygoogletranslation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Rteqe0LXbjYN",
        "outputId": "1cf35722-b6f9-4a12-f596-b9b2628d7dca"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'शुभ प्रभात'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from pygoogletranslation import Translator\n",
        "translator = Translator()\n",
        "translator.translate('Good Morning', dest='hi').text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "zbS4rzFkvDFU",
        "outputId": "624b1e43-6832-443e-e8f9-a1e156593dcc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import pickle\n",
        "from googletrans import Translator\n",
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OFF_tYvXgJ5U"
      },
      "outputs": [],
      "source": [
        "# # translator = Translator(service_urls=['https://translation.googleapis.com/language/translate/v2'])\n",
        "# '''\n",
        "# Translate word . \n",
        "# '''\n",
        "# def translate(word):\n",
        "#     print(word)\n",
        "#     if(len(word)>1 and word!='en' and word!='En' and word!='EN' and word!='on' ):\n",
        "#       translated=translator.translate(word,src='hi' , dest='en').text\n",
        "#       print(translated)\n",
        "#       return translated\n",
        "#     else:\n",
        "#       return word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HgeSRbj4S8lr"
      },
      "outputs": [],
      "source": [
        "def translate(word):\n",
        "    try :\n",
        "      if(len(word)>1 and word!='en'):\n",
        "        return translator.translate(word,src='hi' , dest='en').text\n",
        "      # else:\n",
        "      #   word\n",
        "    except TypeError or JSONDecodeError or Exception:\n",
        "      a = \" \"\n",
        "      return a\n",
        "    else:\n",
        "      return word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "FFzMKNVyTIRu",
        "outputId": "8e00bf73-1b0f-42ed-e361-6479cd879c1c"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Love is cheating'"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "translate(\"pyar dhokha hai\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQimFpoGQFYr"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Self defined contractions\n",
        "''' \n",
        "def load_dict_contractions():\n",
        "    \n",
        "    return {\n",
        "        \"ain't\":\"is not\",\n",
        "        \"amn't\":\"am not\",\n",
        "        \"aren't\":\"are not\",\n",
        "        \"can't\":\"cannot\",\n",
        "        \"'cause\":\"because\",\n",
        "        \"couldn't\":\"could not\",\n",
        "        \"couldn't've\":\"could not have\",\n",
        "        \"could've\":\"could have\",\n",
        "        \"daren't\":\"dare not\",\n",
        "        \"daresn't\":\"dare not\",\n",
        "        \"dasn't\":\"dare not\",\n",
        "        \"didn't\":\"did not\",\n",
        "        \"doesn't\":\"does not\",\n",
        "        \"don't\":\"do not\",\n",
        "        \"e'er\":\"ever\",\n",
        "        \"em\":\"them\",\n",
        "        \"everyone's\":\"everyone is\",\n",
        "        \"finna\":\"fixing to\",\n",
        "        \"gimme\":\"give me\",\n",
        "        \"gonna\":\"going to\",\n",
        "        \"gon't\":\"go not\",\n",
        "        \"gotta\":\"got to\",\n",
        "        \"hadn't\":\"had not\",\n",
        "        \"hasn't\":\"has not\",\n",
        "        \"haven't\":\"have not\",\n",
        "        \"he'd\":\"he would\",\n",
        "        \"he'll\":\"he will\",\n",
        "        \"he's\":\"he is\",\n",
        "        \"he've\":\"he have\",\n",
        "        \"how'd\":\"how would\",\n",
        "        \"how'll\":\"how will\",\n",
        "        \"how're\":\"how are\",\n",
        "        \"how's\":\"how is\",\n",
        "        \"I'd\":\"I would\",\n",
        "        \"I'll\":\"I will\",\n",
        "        \"i'll\":\"I will\",\n",
        "        \"I'm\":\"I am\",\n",
        "        \"I'm'a\":\"I am about to\",\n",
        "        \"I'm'o\":\"I am going to\",\n",
        "        \"isn't\":\"is not\",\n",
        "        \"it'd\":\"it would\",\n",
        "        \"it'll\":\"it will\",\n",
        "        \"it's\":\"it is\",\n",
        "        \"I've\":\"I have\",\n",
        "        \"kinda\":\"kind of\",\n",
        "        \"let's\":\"let us\",\n",
        "        \"mayn't\":\"may not\",\n",
        "        \"may've\":\"may have\",\n",
        "        \"mightn't\":\"might not\",\n",
        "        \"might've\":\"might have\",\n",
        "        \"mustn't\":\"must not\",\n",
        "        \"mustn't've\":\"must not have\",\n",
        "        \"must've\":\"must have\",\n",
        "        \"needn't\":\"need not\",\n",
        "        \"ne'er\":\"never\",\n",
        "        \"o'\":\"of\",\n",
        "        \"o'er\":\"over\",\n",
        "        \"ol'\":\"old\",\n",
        "        \"oughtn't\":\"ought not\",\n",
        "        \"shalln't\":\"shall not\",\n",
        "        \"shan't\":\"shall not\",\n",
        "        \"she'd\":\"she would\",\n",
        "        \"she'll\":\"she will\",\n",
        "        \"she's\":\"she is\",\n",
        "        \"shouldn't\":\"should not\",\n",
        "        \"shouldn't've\":\"should not have\",\n",
        "        \"should've\":\"should have\",\n",
        "        \"somebody's\":\"somebody is\",\n",
        "        \"someone's\":\"someone is\",\n",
        "        \"something's\":\"something is\",\n",
        "        \"that'd\":\"that would\",\n",
        "        \"that'll\":\"that will\",\n",
        "        \"that're\":\"that are\",\n",
        "        \"that's\":\"that is\",\n",
        "        \"there'd\":\"there would\",\n",
        "        \"there'll\":\"there will\",\n",
        "        \"there're\":\"there are\",\n",
        "        \"there's\":\"there is\",\n",
        "        \"these're\":\"these are\",\n",
        "        \"they'd\":\"they would\",\n",
        "        \"they'll\":\"they will\",\n",
        "        \"they're\":\"they are\",\n",
        "        \"they've\":\"they have\",\n",
        "        \"this's\":\"this is\",\n",
        "        \"those're\":\"those are\",\n",
        "        \"'tis\":\"it is\",\n",
        "        \"'twas\":\"it was\",\n",
        "        \"wanna\":\"want to\",\n",
        "        \"wasn't\":\"was not\",\n",
        "        \"we'd\":\"we would\",\n",
        "        \"we'd've\":\"we would have\",\n",
        "        \"we'll\":\"we will\",\n",
        "        \"we're\":\"we are\",\n",
        "        \"weren't\":\"were not\",\n",
        "        \"we've\":\"we have\",\n",
        "        \"what'd\":\"what did\",\n",
        "        \"what'll\":\"what will\",\n",
        "        \"what're\":\"what are\",\n",
        "        \"what's\":\"what is\",\n",
        "        \"what've\":\"what have\",\n",
        "        \"when's\":\"when is\",\n",
        "        \"where'd\":\"where did\",\n",
        "        \"where're\":\"where are\",\n",
        "        \"where's\":\"where is\",\n",
        "        \"where've\":\"where have\",\n",
        "        \"which's\":\"which is\",\n",
        "        \"who'd\":\"who would\",\n",
        "        \"who'd've\":\"who would have\",\n",
        "        \"who'll\":\"who will\",\n",
        "        \"who're\":\"who are\",\n",
        "        \"who's\":\"who is\",\n",
        "        \"who've\":\"who have\",\n",
        "        \"why'd\":\"why did\",\n",
        "        \"why're\":\"why are\",\n",
        "        \"why's\":\"why is\",\n",
        "        \"won't\":\"will not\",\n",
        "        \"wouldn't\":\"would not\",\n",
        "        \"would've\":\"would have\",\n",
        "        \"y'all\":\"you all\",\n",
        "        \"you'd\":\"you would\",\n",
        "        \"you'll\":\"you will\",\n",
        "        \"you're\":\"you are\",\n",
        "        \"you've\":\"you have\",\n",
        "        \"Whatcha\":\"What are you\",\n",
        "        \"whatcha\":\"What are you\",\n",
        "        \"luv\":\"love\",\n",
        "        \"sux\":\"sucks\"\n",
        "        }\n",
        "\n",
        "\n",
        "'''\n",
        "Utility function to handle commonly used short forms\n",
        "'''\n",
        "\n",
        "def handle_short_forms(w):\n",
        "    if w == 'h':\n",
        "        return 'hai'\n",
        "    elif w == 'n':\n",
        "        return 'na'\n",
        "    elif w == 'da':\n",
        "        return 'the'\n",
        "    elif w == 'wid':\n",
        "        return 'with'\n",
        "    elif w == 'pr':\n",
        "        return 'par'\n",
        "    elif w == 'mattt':\n",
        "        return 'mat'\n",
        "    elif w == 'vo':\n",
        "        return 'woh'\n",
        "    elif w == 'ki':\n",
        "        return 'kee'\n",
        "    elif w == 'ap':\n",
        "        return 'aap'\n",
        "    elif w == 'bs':\n",
        "        return 'bas'\n",
        "    elif w == 'goood':\n",
        "        return 'very good'\n",
        "    elif w == 'tera':\n",
        "        return 'teraa'\n",
        "    elif w == 'cnfsn':\n",
        "        return 'confusion'\n",
        "    elif w == 'ka':\n",
        "        return 'kaa'\n",
        "    elif w == 'rkhi':\n",
        "        return 'rakhi'\n",
        "    elif w == 'thts':\n",
        "        return 'thats'\n",
        "    elif w == 'cald':\n",
        "        return 'called'\n",
        "    elif w == 'tabhe':\n",
        "        return 'tabhi'\n",
        "    elif w == 'pta':\n",
        "        return 'pata'\n",
        "    elif w == 'b':\n",
        "        return 'bhi'\n",
        "    elif w == 'nai':\n",
        "        return 'nahi'\n",
        "    elif w == 'f':\n",
        "        return 'of'\n",
        "    elif w == 'd':\n",
        "        return 'the'\n",
        "    else:\n",
        "        return w\n",
        "\n",
        "\n",
        "'''\n",
        "Utility function to clean tweet\n",
        "'''\n",
        "\n",
        "def clean_tweet(tweet):\n",
        "    tweet = tweet.lower()\n",
        "    tweet = re.sub('https.*$', 'URL', tweet)  # remove URLs\n",
        "    tweet = re.sub('RT|cc', '', tweet)  # remove RT and cc\n",
        "    tweet = re.sub('#\\S+', '', tweet)  # remove hashtags\n",
        "    tweet = re.sub('@\\S+', '', tweet)  # remove mentions\n",
        "    tweet = re.sub('[%s]' % re.escape(\"\"\"!\"#$%&'()*+-./:;<=>@[\\]^_`{|}~\"\"\"), ' ', tweet)  # remove punctuations\n",
        "    tweet = re.sub('\\s+', ' ', tweet) \n",
        "    #tweet = word_tokenize(tweet) # remove repeated characters (hellooooo into hello)\n",
        "    # remove extra whitespace\n",
        "    return tweet\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "kc4HcziOxnmW",
        "outputId": "e367113a-6c13-48f7-ad2b-8e5d5735dec0"
      },
      "outputs": [],
      "source": [
        "#Read Training Data\n",
        "train_data=pd.read_csv(\"/content/drive/MyDrive/NLP/emoji.csv\",sep=\"\\t\",header=None)\n",
        "train_data.columns=[\"word\",\"language\",\"sentiment\"]\n",
        "# mentions=train_data.index[(train_data['word']=='@') | (train_data['word']=='#')].tolist()  #list of mention index\n",
        "# mentions_name=list(map(lambda x:x+1,mentions)) \n",
        "# train_data.drop(mentions)\n",
        "# train_data.drop(mentions_name)\n",
        "# mentions_name\n",
        "# len(train_data)\n",
        "train_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "xT0swAfPy79o",
        "outputId": "660c6e3f-1d2a-4d5a-eba9-ea57c00234b9"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>language</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>meta</td>\n",
              "      <td>4330</td>\n",
              "      <td>neutral</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>nen</td>\n",
              "      <td>Eng</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>á</td>\n",
              "      <td>O</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>vist</td>\n",
              "      <td>Eng</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>bolest</td>\n",
              "      <td>Eng</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>166125</th>\n",
              "      <td>t</td>\n",
              "      <td>Eng</td>\n",
              "      <td>NaN</td>\n",
              "      <td>6137</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>166126</th>\n",
              "      <td>.</td>\n",
              "      <td>O</td>\n",
              "      <td>NaN</td>\n",
              "      <td>6137</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>166127</th>\n",
              "      <td>co</td>\n",
              "      <td>Eng</td>\n",
              "      <td>NaN</td>\n",
              "      <td>6137</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>166128</th>\n",
              "      <td>/</td>\n",
              "      <td>O</td>\n",
              "      <td>NaN</td>\n",
              "      <td>6137</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>166129</th>\n",
              "      <td>cS3VTzOp3Q</td>\n",
              "      <td>Eng</td>\n",
              "      <td>NaN</td>\n",
              "      <td>6137</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>166130 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "              word language sentiment  sentence\n",
              "0             meta     4330   neutral         1\n",
              "1              nen      Eng       NaN         1\n",
              "2                á        O       NaN         1\n",
              "3             vist      Eng       NaN         1\n",
              "4           bolest      Eng       NaN         1\n",
              "...            ...      ...       ...       ...\n",
              "166125           t      Eng       NaN      6137\n",
              "166126           .        O       NaN      6137\n",
              "166127          co      Eng       NaN      6137\n",
              "166128           /        O       NaN      6137\n",
              "166129  cS3VTzOp3Q      Eng       NaN      6137\n",
              "\n",
              "[166130 rows x 4 columns]"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Data Allignment\n",
        "train_data[\"sentence\"]=(train_data.word==\"meta\").cumsum()\n",
        "train_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fusv6aHO9w-u"
      },
      "outputs": [],
      "source": [
        "train_dataX=train_data[train_data.word!=\"meta\"]\n",
        "sentiment=train_data.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fmttkAao-Cns",
        "outputId": "03f78267-53d5-4f37-ab6e-a6f02f835574"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "159993"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(train_dataX)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sc5R2zGt_grD",
        "outputId": "6d3da929-8913-4ce6-cc37-d2917f0209cd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        }
      ],
      "source": [
        "#check for consecutive same language word\n",
        "train_dataX['key']=(train_dataX['language']!=train_dataX['language'].shift(1)).astype(int).cumsum() \n",
        "train_dataX['word']=train_data.word.astype(str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wb4yOhWFGf0-"
      },
      "outputs": [],
      "source": [
        "#combining words of english and hindi for a perticular tweet\n",
        "word_cluster=pd.DataFrame(train_dataX.groupby(['key','language'])['word'].apply(' '.join).reset_index())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_Jj_OIiIp4t",
        "outputId": "69fe6e8a-35f8-4179-ab5f-8411430619e2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "77318"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(pd.DataFrame(train_dataX.groupby(['key','language'])['word'].apply(' '.join).reset_index())) #len of word cluster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "ktUhxnhUHFHV",
        "outputId": "490b2257-c924-4f38-95b2-51a454bf6c64"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>key</th>\n",
              "      <th>language</th>\n",
              "      <th>word</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Eng</td>\n",
              "      <td>nen</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>O</td>\n",
              "      <td>á</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Eng</td>\n",
              "      <td>vist bolest vztek smutek</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>Hin</td>\n",
              "      <td>zmatek osam</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>O</td>\n",
              "      <td>ě</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>77313</th>\n",
              "      <td>77314</td>\n",
              "      <td>Eng</td>\n",
              "      <td>t</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>77314</th>\n",
              "      <td>77315</td>\n",
              "      <td>O</td>\n",
              "      <td>.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>77315</th>\n",
              "      <td>77316</td>\n",
              "      <td>Eng</td>\n",
              "      <td>co</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>77316</th>\n",
              "      <td>77317</td>\n",
              "      <td>O</td>\n",
              "      <td>/</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>77317</th>\n",
              "      <td>77318</td>\n",
              "      <td>Eng</td>\n",
              "      <td>cS3VTzOp3Q</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>77318 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         key language                      word\n",
              "0          1      Eng                       nen\n",
              "1          2        O                         á\n",
              "2          3      Eng  vist bolest vztek smutek\n",
              "3          4      Hin               zmatek osam\n",
              "4          5        O                         ě\n",
              "...      ...      ...                       ...\n",
              "77313  77314      Eng                         t\n",
              "77314  77315        O                         .\n",
              "77315  77316      Eng                        co\n",
              "77316  77317        O                         /\n",
              "77317  77318      Eng                cS3VTzOp3Q\n",
              "\n",
              "[77318 rows x 3 columns]"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "word_cluster"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNQZirjIJDHJ"
      },
      "source": [
        "##Translation part "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "-f2IHan5Poul",
        "outputId": "9ab8797c-df34-40f4-8709-ec2fe667371b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>key</th>\n",
              "      <th>language</th>\n",
              "      <th>word</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>38000</th>\n",
              "      <td>38001</td>\n",
              "      <td>Eng</td>\n",
              "      <td>https</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38001</th>\n",
              "      <td>38002</td>\n",
              "      <td>O</td>\n",
              "      <td>//</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38002</th>\n",
              "      <td>38003</td>\n",
              "      <td>Eng</td>\n",
              "      <td>t</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38003</th>\n",
              "      <td>38004</td>\n",
              "      <td>O</td>\n",
              "      <td>.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38004</th>\n",
              "      <td>38005</td>\n",
              "      <td>Eng</td>\n",
              "      <td>co</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47995</th>\n",
              "      <td>47996</td>\n",
              "      <td>Hin</td>\n",
              "      <td>Bechara</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47996</th>\n",
              "      <td>47997</td>\n",
              "      <td>O</td>\n",
              "      <td>.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47997</th>\n",
              "      <td>47998</td>\n",
              "      <td>Eng</td>\n",
              "      <td>https</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47998</th>\n",
              "      <td>47999</td>\n",
              "      <td>O</td>\n",
              "      <td>//</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47999</th>\n",
              "      <td>48000</td>\n",
              "      <td>Eng</td>\n",
              "      <td>t</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10000 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         key language     word\n",
              "38000  38001      Eng    https\n",
              "38001  38002        O       //\n",
              "38002  38003      Eng        t\n",
              "38003  38004        O        .\n",
              "38004  38005      Eng       co\n",
              "...      ...      ...      ...\n",
              "47995  47996      Hin  Bechara\n",
              "47996  47997        O        .\n",
              "47997  47998      Eng    https\n",
              "47998  47999        O       //\n",
              "47999  48000      Eng        t\n",
              "\n",
              "[10000 rows x 3 columns]"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "word_cluster=word_cluster[38000:48000] # word_cluster=word_cluster[48000:58000]\n",
        "word_cluster"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDnpPwh9JH_K"
      },
      "source": [
        "##This part takes time to run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-IlAHBQkSjT",
        "outputId": "60de608d-65fd-4c2a-fec4-f2d179810dc3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2446"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "hindi_word_cluster=word_cluster[(word_cluster[\"language\"]==\"Hin\")]\n",
        "len(hindi_word_cluster)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eefKdFXuHHil"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Applying translations on chunks of words that are tagged as hindi\n",
        "Loading this into eng_word_cluster if already done\n",
        "'''\n",
        "hindi_word_cluster=word_cluster[(word_cluster[\"language\"]==\"Hin\")]\n",
        "# with open(\"english_cluster.pkl\",\"rb\") as input_file:\n",
        "#   eng_word_cluster=pickle.load(input_file)\n",
        "# eng_word_cluster=word_cluster[(word_cluster[\"language\"]==\"Eng\")]\n",
        "eng_word_cluster = hindi_word_cluster[\"word\"].apply(translate)\n",
        "eng_word_cluster_reset=pd.DataFrame(eng_word_cluster)\n",
        "eng_word_cluster_reset=eng_word_cluster_reset.reset_index()\n",
        "eng_word_cluster_reset.columns=['key','word']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPm_13dkJMvN"
      },
      "source": [
        "##from here on all is ok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "raI4K4ampcFG"
      },
      "outputs": [],
      "source": [
        "hindi_word_cluster_r=hindi_word_cluster.reset_index()\n",
        "hindi_word_cluster_r\n",
        "eng_word_cluster\n",
        "hindi_word_cluster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vfX_SrzFqURE"
      },
      "outputs": [],
      "source": [
        "df_translated = pd.DataFrame(columns=['key','word'],\n",
        "                  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WVqft-qLThmp"
      },
      "outputs": [],
      "source": [
        "count=0\n",
        "for i,j in zip(hindi_word_cluster['key'],hindi_word_cluster['word']):\n",
        "    print(i,j)\n",
        "    df_translated\n",
        "    \n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PRSjZ0LSZJId"
      },
      "outputs": [],
      "source": [
        "hindi_word_cluster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ujsb0aBUNvHW"
      },
      "outputs": [],
      "source": [
        "\n",
        "translate('kutta')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2cMUuN66ivJc"
      },
      "outputs": [],
      "source": [
        "eng_word_cluster_reset.columns=['key','word']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "czxL5HmXU3-t"
      },
      "outputs": [],
      "source": [
        "eng_word_cluster_reset.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4yzlX_sslpO6"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Joining mixed code and translated version into result\n",
        "'''\n",
        "eng_word_cluster_reset['key'] = eng_word_cluster_reset['key']+1\n",
        "result = pd.merge(word_cluster, eng_word_cluster_reset, on='key',how='left')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aY0Uh9I7M-qb"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Handling short forms and contractions in the sentences \n",
        "'''\n",
        "long_form_dict = load_dict_contractions()\n",
        "def expand_sent(sentence):\n",
        "    final_sent =\"\"\n",
        "    res = \" \".join(long_form_dict.get(ele, ele) for ele in sentence.split()) \n",
        "    for word in res.split():\n",
        "        final_sent += (handle_short_forms(word))+ \" \"\n",
        "    return final_sent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2of029i0Wfks"
      },
      "outputs": [],
      "source": [
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YVSNSzAHlr1h"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Formatting the dataframe , reset indexes for joining properly\n",
        "'''\n",
        "result = result.reset_index()\n",
        "result.word_y.fillna(result.word_x, inplace=True)\n",
        "#result =result.drop(['level_0','index','word_x'],axis=1)\n",
        "result =result.drop(['index','word_x'],axis=1)\n",
        "original = pd.merge(word_cluster,train_dataX[['sentence','key']],on=\"key\")\n",
        "converted = pd.merge(result,train_dataX[['sentence','key']],on=\"key\")\n",
        "original =original.drop_duplicates()\n",
        "converted =converted.drop_duplicates()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YzkBRkDbB7lw"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Final processed data without extra features\n",
        "'''\n",
        "g = converted.groupby(\"sentence\")\n",
        "h = original.groupby(\"sentence\")\n",
        "#final =  pd.concat([pd.DataFrame(g).reset_index(), pd.DataFrame(h).reset_index()], axis=1, ignore_index=True)\n",
        "#g.apply(lambda sdf: \" \".join(sdf.word))\n",
        "#g.apply(lambda sdf: \" \".join(sdf[sdf.is_nounverb].word))\n",
        "df1 = pd.DataFrame({\n",
        "          \"sent\": h.apply(lambda sdf: \" \".join(sdf.word.astype(str))),\n",
        "          #\"lang\": g.apply(lambda sdf: \" \".join(sdf.lang.astype(str))),\n",
        "          \n",
        "  })\n",
        "df2 = pd.DataFrame({\n",
        "          \"sent\": g.apply(lambda sdf: \" \".join(sdf.word_y.astype(str))),\n",
        "          #\"lang\": g.apply(lambda sdf: \" \".join(sdf.lang.astype(str))),\n",
        "          \n",
        "  })\n",
        "df1 = df1.reset_index()\n",
        "df2 = df2.reset_index()\n",
        "final = pd.merge(df1,df2,on=\"sentence\", validate=\"one_to_one\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lQ6eNZqpCfIj"
      },
      "outputs": [],
      "source": [
        "final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "azeACiH7Cg7c"
      },
      "outputs": [],
      "source": [
        "sentiment = sentiment.reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QG68EBVKD9I0"
      },
      "outputs": [],
      "source": [
        "sentiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eVbXX21PD-58"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Final data with sentiment\n",
        "'''\n",
        "df_merged=pd.concat([final,sentiment.sentiment],axis=1,ignore_index=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XKYXvkuzEDmw"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Rename columns\n",
        "'''\n",
        "df_merged = df_merged.rename(columns={0: \"id\", 1: \"sentence_mixed\", 2:\"sentence_eng\",3:\"sentiment\"})\n",
        "df_merged= df_merged.drop(columns=\"id\")\n",
        "df_merged"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clGnfwNCHoE7"
      },
      "outputs": [],
      "source": [
        "df_merged=df_merged[0:len(final)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vwrvud67EGnc"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Expand sentences , replace words \n",
        "'''\n",
        "df_merged['sentence_mixed'] = df_merged['sentence_mixed'].apply(expand_sent)\n",
        "df_merged['sentence_eng'] = df_merged['sentence_eng'].apply(expand_sent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ucI_cR8OEMvq"
      },
      "outputs": [],
      "source": [
        "df_merged"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AjZteyELQit0"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Clean sentences\n",
        "'''\n",
        "df_merged['sentence_mixed'] = df_merged['sentence_mixed'].apply(clean_tweet)\n",
        "df_merged['sentence_eng'] = df_merged['sentence_eng'].apply(clean_tweet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "egqzP8LZE5G0"
      },
      "outputs": [],
      "source": [
        "df_merged"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jj5YSz0BQ-t4"
      },
      "outputs": [],
      "source": [
        "pd.options.display.max_colwidth = 400\n",
        "with open(r\"drive/My Drive/NLP/processed_english_word_cluster_28000-38000.pkl\", \"wb\") as output_file: #processed_english_word_cluster_48000-58000.pkl\n",
        "    # pick_insert = open('drive/My Drive/NLP/data.pickle','wb')\n",
        "    pickle.dump(df_merged, output_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X_DuDUu__Oor"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z2W3roz4_Ois"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lLwFY6OX_Od3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b5MKvzvM_ObK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I5iXnx1THyit"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Hindi-English.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
